# LLM-Powered AWS Tutor

This project is an MVP demonstrating a Retrieval-Augmented Generation (RAG) approach
to building a chat-based AWS Tutor using GPT and AWS docs.

## Structure
- **backend**: Python-based REST API (FastAPI/Flask) containerized with Docker.
- **ingestion**: Scripts for scraping/embedding AWS docs and storing them in a vector DB.
- **frontend**: React (or Next.js) app for the chat UI, containerized with Docker.
- **k8s**: Kubernetes manifests for deploying on AWS EKS.




## Environment Variables

**Overview**
Securing environment variables is crucial for maintaining the integrity and security of my application. In my project, I require two primary databases:
- Relational Database: To store structured data such as user information, chat history, configurations, etc.
- Vector Database: To store and manage embeddings generated by my ingestion process for efficient retrieval.

I'll use:
- PostgreSQL for the Relational Database.
- Weaviate for the Vector Database.

**Relational Database**
- Setting Up the Relational Database (PostgreSQL)
  - Why PostgreSQL?
    - PostgreSQL is a powerful, open-source relational database system known for its robustness, scalability, and support for advanced data types. It's ideal for handling structured data and complex queries.
  - Options for Hosting PostgreSQL
    - AWS RDS (Managed Service)
    - Docker (Local Development)
    - Self-Hosted on a Server
  - For this guide, we'll cover both Docker for local development and AWS RDS for production deployment.


